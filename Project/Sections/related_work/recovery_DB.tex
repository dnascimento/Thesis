%!TEX encoding = UTF-8 Unicode\subsection{Recovery at Database Level}\label{sec:recovery_database}A vast number of database management systems (DBMS) support recovery by loading a snapshot (or full-backup) of the database, possibly patched with blocks of data modified since the snapshot was taken. However, this approach does not distinguish between malicious and legitimate requests. The recovery approach we are interested in the document is applied to databases similarly to what was seen in Section \ref{sec:recovery_os} for operating systems. While the operating system dependencies are established by the system calls, the database dependencies are established by transactions. First, we define the main dependencies in the database systems. These rules are used by most of the recovery services for databases and applications (Section \ref{sec:recApp}).\\Compromised transactions are determined from an initial set of bad transactions using read and write rules. ``Transaction $ T_j$ is dependent upon transaction $ T_i$ if there is a data item $x$ such that $T_j$ reads $x$" \cite{Ammann2002} and $T_i$ performs the latest update $x$. Transaction dependency is transitive. ``A good transaction $G_i$ is suspected if some bad transaction $B_i$ affects $G_i$. A data item $x$ is compromised if $x$ is written by any bad or suspect transaction" \cite{Ammann2002}. This dependency chain is broken if a transaction performs a blind write, i.e., the transaction writes an item without read it first.However, legitimate transactions can have different outputs even when their original execution were not tainted. For example, a malicious transaction can remove a data entry which the following transactions would read and then write other data entries. Since user mistakes are often deletes due to wrong query arguments, this is a relevant issue. Xie \textit{et al.} \cite{Xie2008} propose to track the transaction that deleted the data entries keeping a copy of deleted data entries in a separated database table. To add the dependencies from the deleted data items, the SQL statement is performed in the original and delete tracking tables. The dependency rules require to extract the read and the write set of each transaction, i.e., the set of data entries that each transaction read or modifies. The following proposals use different methods to extract these sets and restore the tainted data entries.\\\textbf{ITDB \cite{itdb,Ammann2002,Wang2007}:} Intrusion Tolerant Database (ITDB) performs intrusion recovery in databases using compensating and supports \textit{runtime recovery}, i.e., the database service remains available during the recovery process. ITDB uses the generic set of dependency rules mentioned in the beginning of Section \ref{sec:recovery_database} and extracts the read and write set parsing the SQL statements.During the record phase, ITDB audits the read and write sets of each transaction. Most relational DBMS only keep write logs. Therefore, Liu \textit{et al}. \cite{itdb} propose a pre-defined per-transaction type template to extract the read set of parsed SQL statements. This approach is application dependent since updates in application queries require updates in their templates. %algoritmo de recoveryAt recovery phase, ITDB initiates a set $D_{tainted}$ with the intrusion source data items. Then, it reads the log of read and write sets of each transaction from the intrusion moment to the present. For each transactions, ITDB keeps the write set until the transaction commits or aborts; if the transaction commits after reading some data item that belongs to $D_{tainted}$, ITDB adds the data items in the transaction write set to $D_{tainted}$ and the transaction must be compensated. The compensating of a transaction reverts the effects of the original transaction. It performs the inverse modification of the original transaction to restore the previous values. Repaired entries in $D_{tainted}$ are tracked to prevent compensating of later transactions from restore a repaired data entry to its version after the attack. After this process, the latest legitimate value of $D_{tainted}$ entries is recovered.%extrasIf the intrusion propagation is faster than the recovery process then the recovery phase is endless because the damage will spread through new transactions. To prevent damage spreading, ITDB blocks the read accesses to the data entries in $D_{tainted}$. Since identification of $D_{tainted}$ requires log analysis, Liu \textit{et al.} propose a \textit{multi-phase damage container} technique to avoid damage spread through new requests during the recovery phase. This damage containing approach denies the read access to the data items were written after the intrusion. Then, during the recovery phase, it releases the data entries that were mistakenly contained. This approach speeds-up the recovery phase and confines the damaged data items. More, it supports \emph{runtime recovery}. However it decreases the system availability during the recovery period and degrades the performance.An alternative approach concerns the throughput asymmetry between the recovery and the user flows. The asymmetry can be neutralized if the repair request priority is increased. Then, the availability is not compromised anymore but users may read tainted data and propagate the damage slower. The ITDB architecture includes an intrusion detection system. The IDS is application aware and acts at transaction level. Liu \textit{et al.} propose isolation in terms of users: when a suspicious transaction is reported by the IDS, every write operation from the suspicious user is done in isolated tables.The ITDB does not perform transaction replay during the recovery phase. Therefore, it ignores that legitimate executions can be influenced by the updated values. Liu \textit{et al.} propose a theoretical model \cite{Yu2003} based on possible workflows and versioning. However, predict every possible workflow requires extensive computational and storage resources. \\\textbf{Phoenix \cite{phoenix}:} Phoenix removes the intrusion effects using a versioned database. While Liu \textit{et al.} \cite{Ammann2002} rely on templates of SQL statements and read the log in recovery time, Phoenix changes the DBMS code to extract read dependencies and proposes a runtime algorithm to check dependencies between transactions.%VersioningPhoenix performs every write operation appending a new row to the table. This new row includes an unique transaction id to support the restoration of previous row versions. Phoenix modifies the PostgreSQL DMBS code to intercept read queries during their execution and to extract the transaction id of each accessed row. The logged data is used to update the dependency graph.%Como funcionaAt the recovery process, Phoenix identifies the set of affected transactions, $A_{tainted}$, from a root set of malicious transactions, $A_{intrusion}$, using the dependency graph. Then, it changes these transactions status to \textit{abort} in the PostgreSQL transaction log. Since PostgreSQL, in serializable snapshot isolation mode, exposes only the row version of the latest non-aborted transaction, the row is restored and the effect of tainted transactions are removed.\\\textbf{Summary:} Recovery systems for relational databases differ on their methods to track the read and write sets and to restore previous values. ITDB is application dependent because it requires pre-defined templates to parse the SQL statements. On the other hand, Phoenix is application independent but DBMS dependent because it modifies the DBMS source code and relies on the usage of serializable snapshot isolation mode. ITDB is records the previous versions in contrast to Phoenix which just records the transactions. The first requires more storage capacity to save the versions, the second requires the knowledge of the compensating of each transaction and more computation resources during recovery to revert the tainted transactions.The data item granularity affects the runtime performance overhead and the accuracy of dependency tracking. Coarser granularity, e.g., row, results in lower performance overhead but a higher probability of false dependence if two transactions read and write different portions of the same data item. Moreover, an attack can compromise just an independent part of the transaction and legitimate data is removed. 