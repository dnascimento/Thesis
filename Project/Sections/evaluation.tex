%!TEX encoding = UTF-8 Unicode
%- Como pensam testar a solucao e validar a vossa idea, que processo vao utilizar durante o desenvolvimento para facilitar a rrealizacao de testes e a recolha de resultados.
%- Pretendo usar um prototipo: que equipamentos/linguagens de programacao a usar
\section{Evaluation}
\label{sec:Evaluation}

The evaluation of the proposed architecture will be done experimentally, building a prototype. This evaluation aims to validate Shuttle and to prove its scalability in large implementations. The validation aims to show that Shuttle allows to recover from attacks against at least five of the OWASP Top 10 Application Security Risks: injection flaws, broken authentication, security misconfiguration, missing function level access control and using components with vulnerabilities \cite{Williams2013}. Shuttle should achieve the proposed goals and its results should match the evaluation metrics bounds which allows the Shuttle usage in real environments. We evaluate our proposal by how effective and how efficient it is, i.e., the false positive and false negative rates, the record and recovery overhead and the availability during the recovery process. In detail, we intend to measure the following:

\begin{itemize}
  \item \textbf{Recording:} We want to quantify the \textit{logging overhead} imposed measuring the \textit{delay} per request and system \textit{throughput}, \textit{resources usage} and \textit{maximum load} variance due to recording mechanisms comparing with common execution, the \textit{log size} needed to recover the system from a previous snapshot. We also will measure the time required to perform a system snapshot.
  \item \textbf{Recovery:} We aim to measure the effectiveness in terms of \textit{precision} and \textit{recall}. Precision is the fraction of the updated items that are correctly updated. Recall represents the proportion of tainted items updated out of the total number of tainted items. We can measure the number of false positives from precision, and the number of false positives from the recall. 
  We will check the recovery scalability and measure its duration for different numbers of requests, checkpoints and dependencies.
  \item \textbf{Integrity and availability:} We will measure and trace the percentage of corrupted data items and the percentage of available data items during the recovery process. These values help to define the application availability during the recovery process.
  \item \textbf{Consistent replay:} We want to guarantee that our service provides the same results as original execution if the requests and application code remain identical, even when supporting parallel and concurrent requests. 
  \item \textbf{Parallel replay:} We will measure the performance changes when the replay is performed in parallel.
  \item \textbf{Cost:} We will measure the monetary cost of the intrusion recovery process using a public cloud providers.
\end{itemize}

We will do the measurements taking into account varying sizes of the state and time between snapshots, the attack type, the damage created, the detection delay, the request arrival rate. More, we will validate if the application integrity is enough to support an adequate service during the recovery process in the hope of providing an intrusion tolerant application.

We will develop a demo web application which will be deployed on a PaaS system. Since independent user sessions would be trivial to replay, the application will have highly dependent interactions between requests from different users. The application will be a Java Spring \cite{spring} implementation of a Question and Answering (Q&A) application, like Stack Overflow \cite{stackoverflow} and Yahoo! Answers \cite{yahooAnswers}. Spring is one of the most used Java enterprise web frameworks and it is compatible with most of the current PaaS systems. To highlight the Shuttle scalability, the application will store its state in the NoSQL database Voldemort \cite{Kreps}.\\

We will develop a \emph{testing tool} to evaluate the service using HTTP requests from multiple nodes coordinated by a master node. The tool aims to simulate a real application load from multiple clients. To evaluate the efficiency, the tool will measure the response time and throughput of each user. To evaluate the effectiveness, the tool will analyze the consistency of the application responses during the recovery phase. The results will be extracted from a set of intrusion scenarios. The scenarios will include accidentally or malicious requests, external actions, e.g. ssh sessions, that compromise the PaaS containers. The scenarios will also include software updates where previous software flaws are fixed. These intrusion scenarios will be created according to typical application vulnerabilities. We will compare the overhead during normal and recovery operations. 

The prototype evaluation is branched in private cloud and public cloud. Due to public cloud costs, we will implement a test prototype on a private cloud and evaluate the final prototype on a public cloud. First, our prototype will be deployed in a PaaS system provided by AppScale \cite{Appscale} or OpenShift \cite{OpenShift}. The open source PaaS systems will run over OpenStack \cite{openstack}. Later, we will perform the final evaluation running the application over Amazon Web Service IaaS \cite{aws} or Google Cloud Platform \cite{googleCloudPlatform} to overcome our limited resources and scale to medium enterprise size scenarios.
